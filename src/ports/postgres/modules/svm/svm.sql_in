/* ----------------------------------------------------------------------- *//**
 *
 * @file svm.sql_in
 *
 * @brief SQL functions for GLM (Poisson)
 * @date June 2014
 *
 * @sa For a brief introduction to GLM (Poisson), see the
 *     module description \ref grp_poisson.
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/**
@addtogroup grp_svm



@brief Solves classification and regression problems by separating the data with a hyperplane or other nonlinear 
decision boundaries.


Support Vector Machines (SVMs) are a commonly used technique for approaching regression and classification problems. SVM models have two particularly desirable features: robustness in the presence of noisy data, and applicability to a variety of data schemes. At its core, a <em>linear</em> SVM model utilizes a hyperplane to separate input data or determine regression values. Using kernels, one can utilize the same techniques with non-linear decision boundaries.



@anchor train
@par Training Function
SVM training function has the following format:
<pre class="syntax">
svm_train(source_table,
    model_table,
    dependent_varname,
    independent_varname,
    kernel_func,
    kernel_params,
    optim_params,
    reg_params
    verbose
    )
</pre>
\b Arguments
<DL class="arglist">
  <DT>source_table</DT>
  <DD>TEXT. The name of the table containing the training data.</DD>

  <DT>model_table</DT>
  <DD>TEXT. Name of the output table containing the model.

    The model table produced by svm contains the following columns:

    <table class="output">
      
      <tr>
        <th>coef</th>
        <td>FLOAT8. Vector of the coefficients.</td>
      </tr>

      <tr>
        <th>bias</th>
        <td>FLOAT8. The bias term.</td>
      </tr>

      <tr>
        <th>nSV</th>
        <td>INTEGER. Number of support vectors.</td>
      </tr>

      <tr>
        <th>kernel_func</th>
        <td>TEXT. Type of kernel being modeled.</td>
      </tr>

      <tr>
        <th>kernel_parameters</th>
        <td>TEXT. Parameters of the kernel function. In case of non-linear kernel, also include random feature map data.</td>
      </tr>

	  <tr>
        <th>num_iterations</th>
        <td>INTEGER. Number of iterations actually completed.</td>
      </tr>

	</table>
	
	A summary table named \<model_table\>_summary is also created at the same time, which has the following columns:
    <table class="output">

    <tr>
    <th>method</th>
    <td>'svm'</td>
    </tr>

    <tr>
    <th>source_table</th>
    <td>The data source table name.</td>
    </tr>

    <tr>
    <th>model_table</th>
    <td>The model table name.</td>
    </tr>

    <tr>
    <th>dependent_varname</th>
    <td>The dependent variable.</td>
    </tr>

    <tr>
    <th>independent_varname</th>
    <td>The independent variables</td>
    </tr>

    <tr>
    <th>kernel_func</th>
    <td>The kernel function</td>
    </tr>

    <tr>
    <th>kernel_parameters</th>
    <td>The kernel parameters, as well as random feature map data</td>
    </tr>

    <tr>
    <th>optim_params</th>
    <td>A string containing the optimization parameters</td>
    </tr>

    <tr>
    <th>reg_params</th>
    <td>A string containing the regularization parameters</td>
    </tr>

	<tr>
      <th>num_rows_processed</th>
      <td>BIGINT. Total numbers of rows processed in all groups.</td>
    </tr>

    <tr>
      <th>num_rows_skipped</th>
      <td>BIGINT. Total numbers of rows skipped in all groups due to missing values or failures.</td>
    </tr>

   </table>



  </DD>

  <DT>dependent_varname</DT>
  <DD>TEXT. Name of the dependent variable column.</DD>

  <DT>independent_varname</DT>
  <DD>TEXT. Expression list to evaluate for the
    independent variables. An intercept variable is not assumed. It is common to
    provide an explicit intercept term by including a single constant \c 1 term in
    the independent variable list.</DD>

  <DT>kernel_func</DT>
  <DD>TEXT, default: 'linear'
    The type of kernel. Currently, three types are supported: linear, RBF, and Polynomial.</DD>

  <DT>kernel_params</DT>
  <DD>TEXT, default: NULL
   Parameters for non-linear kernel. If RBF, one parameter, \f$\gamma\f$, is expected. If polynomial, two parameters are expected:
    a coefficient \f$q\f$ and degree \f$d\f$, i.e. \f$ (\langle x,y\rangle + q)^d \f$.</DD>

  <DT>optim_params</DT>
  <DD>TEXT, default: ''
    Optimization paramters, such as stepsize, max number of iterations, etc.</DD>

  <DT>reg_params</DT>
  <DD>TEXT, default: 'linear'
    Regularization parameters, such as \f$\lambda\f$.</DD>

</DL>


@anchor predict
@par Prediction Function
The prediction function is provided to estimate the conditional mean given a new
predictor. It has the following syntax:
<pre class="syntax">
svm_predict(model_table,
            new_data_table,
            output_table)
</pre>

\b Arguments
<DL class="arglist">
  <DT>model_table</DT>
  <DD>TEXT. Model table produced by the training function.</DD>

  <DT>new_data_table</DT>
  <DD>TEXT. Name of the table containing prediction data. This table is expected to contain the same features that were used during training. The table should also contain id_col_name used for identifying each row.</DD>

  <DT>output_table</DT>
  <DD>TEXT. Name of the table to output prediction results to. If this table already exists then an error is returned. The table contains the id_col_name column giving the 'id' for each prediction and the prediction columns for the dependent variable.</DD>
</DL>




@anchor background
@par Technical Background

To solve linear SVM, the following objective function is minimized:

\f[
    \underset{w,b}{\text{Minimize }} \frac{1}{2}||w||^2 + \frac{C}{n}\sum_{i=1}^n \ell(y_i,f_{w,b}(x_i))

\f]
 
 where \f$(x_1,y_1),\ldots,(x_n,y_n)\f$ are labeled training data and \f$\ell(y,f(x)) = \max(0,1-yf(x))\f$ is the <em>hinge loss</em>. If \f$ f_{w,b}(x) = \langle w, x\rangle + b\f$, for example, then the objective function is convex and incremental gradient descent (IGD, or SGD) can be applied to find a global minimum.



To learn with RBF or polynomial kernels, the training data is first mapped via a <em>random feature map</em> in such a way that the usual inner product in the range space approximates the kernel function in the original feature space. The linear SVM training function is then run on the resulting data. See the papers [1,2] for more information on random feature maps.

Also, see the book [3] by Scholkopf and Smola  for more details of SVMs in general.

@anchor literature
@literature

@anchor svm-lit-1
[1] Purushottam Kar and Harish Karnick: Random Feature Maps for Dot
	Product Kernels, Proceedings of the 15th International Conference
	on Artificial Intelligence and Statistics, 2012,
	http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2012_KarK12.pdf

@anchor svm-lit-2
[2] Ali Rahmini and Ben Recht: Random Features for Large-Scale 
Kernel Machines, Neural Information Processing Systems 2007,
	http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf

@anchor svm-lit-3
[3] Bernhard Scholkopf and Alexander Smola: Learning with Kernels,
	The MIT Press, Cambridge, MA, 2002.

   

@anchor related
@par Related Topics

File svm.sql_in documenting the training function



@internal
@sa Namespace SVM (documenting the driver/outer loop implemented in
    Python), Namespace
    \ref madlib::modules::regress documenting the implementation in C++
@endinternal
*/


