/* ----------------------------------------------------------------------- *//**
 *
 * @file svm.sql_in
 *
 * @brief SQL functions for GLM (Poisson)
 * @date June 2014
 *
 * @sa For a brief introduction to GLM (Poisson), see the
 *     module description \ref grp_poisson.
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/**
@addtogroup grp_svm



@brief Solves classification and regression problems by separating the data with a hyperplane or other nonlinear 
decision boundaries.


Support Vector Machines (SVMs) are a commonly used technique for approaching regression and classification problems. SVM models have two particularly desirable features: robustness in the presence of noisy data, and applicability to a variety of data schemes. At its core, a <em>linear</em> SVM model utilizes a hyperplane to separate input data or determine regression values. Using kernels, one can utilize the same techniques with non-linear decision boundaries.



@anchor train
@par Training Function
SVM training function has the following format:
<pre class="syntax">
svm_train(source_table,
    model_table,
    dependent_varname,
    independent_varname,
    kernel_func,
    kernel_params,
    grouping_col,
    optim_params,
    reg_params,
    verbose
    )
</pre>
\b Arguments
<DL class="arglist">
  <DT>source_table</DT>
  <DD>TEXT. The name of the table containing the training data.</DD>

  <DT>model_table</DT>
  <DD>TEXT. Name of the output table containing the model.

    The model table produced by svm contains the following columns:

    <table class="output">
      
      <tr>
        <th>coef</th>
        <td>FLOAT8. Vector of the coefficients.</td>
      </tr>

      <tr>
        <th>random_feature_data</th>
        <td>BYTEA8. Data needed to embed test data into random feature space. NULL if the kernel is linear.</td>
      </tr>

	  <tr>
    	<th>grouping_col</th>
    	<td>Name of grouping columns.</td>
      </tr>

      <tr>
        <th>num_rows_processed</th>
        <td>BIGINT. Numbers of rows processed.</td>
      </tr>

      <tr>
        <th>num_rows_skipped</th>
        <td>BIGINT. Numbers of rows skipped due to missing values or failures.</td>
      </tr>

	  <tr>
        <th>num_iterations</th>
        <td>INTEGER. Number of iterations actually completed.</td>
      </tr>

	</table>
	
	A summary table named \<model_table\>_summary is also created at the same time, which has the following columns:
    <table class="output">

    <tr>
    <th>method</th>
    <td>'svm'</td>
    </tr>

    <tr>
    <th>source_table</th>
    <td>The data source table name.</td>
    </tr>

    <tr>
    <th>model_table</th>
    <td>The model table name.</td>
    </tr>

    <tr>
    <th>dependent_varname</th>
    <td>The dependent variable.</td>
    </tr>

    <tr>
    <th>independent_varname</th>
    <td>The independent variables.</td>
    </tr>

    <tr>
    <th>kernel_func</th>
    <td>The kernel function.</td>
    </tr>

    <tr>
    <th>kernel_parameters</th>
    <td>The kernel parameters, as well as random feature map data.</td>
    </tr>

    <tr>
    <th>grouping_col</th>
    <td>Columns on which to group.</td>
    </tr>

    <tr>
    <th>optim_params</th>
    <td>A string containing the optimization parameters.</td>
    </tr>

    <tr>
    <th>reg_params</th>
    <td>A string containing the regularization parameters.</td>
    </tr>

	<tr>
    <th>num_all_groups</th>
    <td>Number of groups in glm training.</td>
    </tr>

    <tr>
    <th>num_failed_groups</th>
    <td>Number of failed groups in glm training.</td>
    </tr>

    <tr>
      <th>total_rows_processed</th>
      <td>BIGINT. Total numbers of rows processed in all groups.</td>
    </tr>

    <tr>
      <th>total_rows_skipped</th>
      <td>BIGINT. Total numbers of rows skipped in all groups due to missing values or failures.</td>
    </tr>

   </table>



  </DD>

  <DT>dependent_varname</DT>
  <DD>TEXT. Name of the dependent variable column. Whether regression or classification is performed
  depends on the type of this variable. If the entries are BOOLEAN, regression is performed, and if the
  the entries are DOUBLE PRECISION, regression will be performed. All other data types will be rejected.
  If the entries are DOUBLE PRECISION but only assume two distinct values, a warning will be issued. </DD>

  <DT>independent_varname</DT>
  <DD>TEXT. Expression list to evaluate for the
    independent variables. An intercept variable is not assumed. It is common to
    provide an explicit intercept term by including a single constant \c 1 term in
    the independent variable list. Expression should be able to be casted into DOUBLE PRECISION [].</DD>

  <DT>kernel_func (optional)</DT>
  <DD>TEXT, default: 'linear'.
    The type of kernel. Currently, three types are supported: 'linear', 'gaussian', and 'polynomial'.</DD>

  <DT>kernel_params (optional)</DT>
  <DD>TEXT, defaults: NULL. If 'linear' kernel, \f$\gamma=\frac{1}{num\_features}\f$ if 'gaussian', and q=0,d=3 if 'polynomial'.
   Parameters for non-linear kernel. If gaussian, one parameter, \f$\gamma\f$, is expected. If polynomial, two parameters are expected:
    a coefficient \f$q\f$ and degree \f$d\f$, as in \f$ (\langle x,y\rangle + q)^d \f$. </DD>

  <DT>grouping_col (optional)</DT>
  <DD>TEXT, default: NULL. An expression list used to group
    the input dataset into discrete groups, running one model per group.
    Similar to the SQL "GROUP BY" clause. When this value is NULL, no
    grouping is used and a single model is generated.</DD>

  <DT>optim_params (optional)</DT>
  <DD>TEXT, default: 'init_stepsize=0.01, decay_factor=0.9 max_iters=100'.
    Optimization paramters, such as stepsize, max number of iterations, etc.</DD>

  <DT>reg_params (optional)</DT>
  <DD>TEXT, default: 'lambda=0.01, n_folds=0, param_explored'.
    Regularization parameters. If the user specifies n_folds >0, then cross-validation with n_folds will be performed to find the
    optimum values of 'param_explored'. n_folds should be > 2, if positive.</DD>

  <DT>verbose (optional)</DT>
  <DD>BOOLEAN default: FALSE.
    Verbose output of the results of training.</DD>

</DL>


@anchor predict
@par Prediction Function
The prediction function is provided to estimate the conditional mean given a new
predictor. It has the following syntax:
<pre class="syntax">
svm_predict(model_table,
            new_data_table,
            id_col_name,
            output_table)
</pre>

\b Arguments
<DL class="arglist">
  <DT>model_table</DT>
  <DD>TEXT. Model table produced by the training function.</DD>

  <DT>new_data_table</DT>
  <DD>TEXT. Name of the table containing prediction data. This table is expected to contain the same features that were used during training. The table should also contain id_col_name used for identifying each row.</DD>

  <DT>id_col_name</DT>
  <DD>TEXT. The name of the id column in the input table.</DD>

  <DT>output_table</DT>
  <DD>TEXT. Name of the table to output prediction results to. If this table already exists then an error is returned. The table contains the id_col_name column giving the 'id' for each prediction and the prediction columns for the dependent variable.</DD>
</DL>




@anchor background
@par Technical Background

To solve linear SVM, the following objective function is minimized:

\f[
    \underset{w,b}{\text{Minimize }} \lambda||w||^2 + \frac{1}{n}\sum_{i=1}^n \ell(y_i,f_{w,b}(x_i))

\f]
 
 where \f$(x_1,y_1),\ldots,(x_n,y_n)\f$ are labeled training data and \f$\ell(y,f(x)) = \max(0,1-yf(x))\f$ is the <em>hinge loss</em>. If \f$ f_{w,b}(x) = \langle w, x\rangle + b\f$, for example, then the objective function is convex and incremental gradient descent (IGD, or SGD) can be applied to find a global minimum. Consult Feng, et al. [1] for more details.



To learn with Gaussian or polynomial kernels, the training data is first mapped via a <em>random feature map</em> in such a way that the usual inner product in the range space approximates the kernel function in the original feature space. The linear SVM training function is then run on the resulting data. See the papers [2,3] for more information on random feature maps.

Also, see the book [4] by Scholkopf and Smola  for more details of SVMs in general.

@anchor literature
@literature

@anchor svm-lit-1
[1] Xixuan Feng, Arun Kumar, Ben Recht, and Christopher Re:
	Towards a Unified Architecture for in-RDBMS analytics,
	in SIGMOD Conference, 2012
	http://www.eecs.berkeley.edu/~brecht/papers/12.FengEtAl.SIGMOD.pdf

@anchor svm-lit-2
[2] Purushottam Kar and Harish Karnick: Random Feature Maps for Dot
	Product Kernels, Proceedings of the 15th International Conference
	on Artificial Intelligence and Statistics, 2012,
	http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2012_KarK12.pdf

@anchor svm-lit-3
[3] Ali Rahmini and Ben Recht: Random Features for Large-Scale 
Kernel Machines, Neural Information Processing Systems 2007,
	http://www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf

@anchor svm-lit-4
[4] Bernhard Scholkopf and Alexander Smola: Learning with Kernels,
	The MIT Press, Cambridge, MA, 2002.

   

@anchor related
@par Related Topics

File svm.sql_in documenting the training function



@internal
@sa Namespace SVM (documenting the driver/outer loop implemented in
    Python), Namespace
    \ref madlib::modules::regress documenting the implementation in C++
@endinternal
*/


